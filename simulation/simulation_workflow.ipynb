{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14ec80f-bfa9-4d3f-b807-2269f3a9d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import product_fem as pf\n",
    "import fenics\n",
    "import json\n",
    "from PIL import Image\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2a85a-6b99-4b7c-a368-bf3fb97ea62a",
   "metadata": {},
   "source": [
    "# Parameters for inference\n",
    "\n",
    "Here are things we might want to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5748a6d-e7e2-4849-80fb-a61eb23befd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_n = 5\n",
    "penalty = 1.0\n",
    "epsilon = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e0b13-d445-4914-bea3-e114b4ea45f8",
   "metadata": {},
   "source": [
    "# Read in the data\n",
    "\n",
    "This reads in:\n",
    "1. spatial statistics (e.g., average density at a grid of points), from `.spstats.csv`\n",
    "2. sampling locations and genetic diversities there, from `.stats.csv`\n",
    "3. pairwise divergences, from `.pairstats.csv`,\n",
    "4. the parameters, from `params.json`\n",
    "\n",
    "and also loads the maps used to parameterize the simulation, as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0640602-1a5e-4a03-934c-f8ceabe90746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spatial and genetic data\n",
    "simbase = \"density_saddle/out_2059675351901\"\n",
    "basename = \"density_saddle/out_2059675351901_stats/rep876970\"\n",
    "basedir = os.path.split(basename)[0]\n",
    "mapfile = f\"{simbase}.spstats.csv\"\n",
    "with open(os.path.join(basedir, \"params.json\"), \"r\") as pfile:\n",
    "    params = json.load(pfile)\n",
    "    for k in params:\n",
    "        if len(params[k]) == 1:\n",
    "            params[k] = params[k][0]\n",
    "spatial_data = pd.read_csv(f\"{basename}.stats.csv\", index_col=0)\n",
    "genetic_data = pd.read_csv(f\"{basename}.pairstats.csv\", index_col=0)\n",
    "bias_map = Image.open(params['BIAS_MAP_FILE'])\n",
    "cov_map = Image.open(params['COVARIANCE_MAP_FILE'])\n",
    "habitat_map = Image.open(params['HABITAT_MAP_FILE'])\n",
    "empirical_maps = pd.read_csv(mapfile)\n",
    "width, height = bias_map.width / params['MAP_RESOLUTION'], bias_map.height / params['MAP_RESOLUTION']\n",
    "aspect_ratio = width/height\n",
    "size = (width + height)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc63ba9-b327-49f0-b40b-d606458f5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749cbd9c-9bc9-425b-a0cc-aa9bf07e6bb8",
   "metadata": {},
   "source": [
    "Here are the images used to parameterize the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce3082-6214-445f-9030-3e9e343bfc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 9))\n",
    "for (im, lab), ax in zip(((bias_map, 'bias'), (cov_map, 'cov'), (habitat_map, 'habitat')), axes):\n",
    "    ax.imshow(im, extent=(0, width, 0, height))\n",
    "    ax.set_title(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b138e44-d98e-478d-b528-2ed5d46254db",
   "metadata": {},
   "source": [
    "# Empirical maps\n",
    "\n",
    "Here are the maps of average density, fecundity, and establishment\n",
    "that SLiM recorded during the simulation.\n",
    "First some helper functions to pull the information out of the pandas table it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb8c167-cce4-4e23-af1b-ac11770eae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_map_array(sp, n):\n",
    "    xvals = np.unique(sp['x'])\n",
    "    yvals = np.unique(sp['y'])\n",
    "    nr, nc = len(xvals), len(yvals)\n",
    "    X = sp['x'].to_numpy().reshape((nr, nc))\n",
    "    for i in range(nr):\n",
    "        assert len(np.unique(X[i,:])) == 1 and X[i,0] == xvals[i]\n",
    "    Y = sp['y'].to_numpy().reshape((nr, nc))\n",
    "    for j in range(nc):\n",
    "        assert len(np.unique(Y[:,j])) == 1 and Y[0, j] == yvals[j]\n",
    "    return xvals, yvals, empirical_maps[n].to_numpy().reshape((nr, nc))\n",
    "\n",
    "def make_map_fn(sp, n):\n",
    "    xvals, yvals, Z = make_map_array(sp, n)\n",
    "    return interpolate.RegularGridInterpolator((xvals, yvals), Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53004cc-90e4-45e9-ae5e-7e7f024fb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 9))\n",
    "for lab, ax in zip((\"density\", \"fecundity\", \"establishment\"), axes):\n",
    "    _, _, im = make_map_array(empirical_maps, lab)\n",
    "    ax.imshow(im, extent=(0, width, 0, height))\n",
    "    ax.set_title(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb8231-f828-4924-a320-8da3c480bce8",
   "metadata": {},
   "source": [
    "Here is some of the data! In this map, circle size is proportional to heterozygosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b910b5-1849-4f1c-816a-992e9a48a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "het_scale = 10**(2-np.floor(np.log10(np.max(spatial_data['het']))))\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(habitat_map, extent=(0, width, 0, height))\n",
    "ax.scatter(spatial_data['x'], spatial_data['y'], s=spatial_data['het']*het_scale, c='pink', edgecolors='black')\n",
    "ax.set_aspect(1.0)\n",
    "ax.set_title(f\"heterozygosity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da568ea3-f51a-4f36-be3d-a9a66ad50176",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now, we'll set up for inference: making a mesh,\n",
    "and normalizing coordinates and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab0450-5382-48b0-a596-7b791f33b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = {\n",
    "    'xy': size,\n",
    "}\n",
    "\n",
    "mesh = fenics.RectangleMesh(fenics.Point(0.0, 0.0), fenics.Point(width/scaling['xy'], height/scaling['xy']),\n",
    "                            int(aspect_ratio * mesh_n), mesh_n)\n",
    "V = fenics.FunctionSpace(mesh, 'CG', 1)\n",
    "W = pf.ProductFunctionSpace(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fa82a-80fe-4f21-a71f-c80d79fbc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "fenics.plot(mesh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d9550-fee0-4bd3-86c2-62568b393f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize dxy\n",
    "train_dxy = genetic_data['dxy'].to_numpy()\n",
    "scaling['dxy'] = train_dxy.max()\n",
    "train_dxy /= scaling['dxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcf177-8d00-447d-b1c1-16bfdaadb7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise coordinates needed for kernel density estimate\n",
    "points = spatial_data[['x', 'y']].to_numpy() / scaling['xy']\n",
    "\n",
    "def coords_to_pairs(points):\n",
    "    n = len(points)\n",
    "    N = int(n * (n + 1) / 2)\n",
    "    xs = np.zeros((N, 2))\n",
    "    ys = np.zeros((N, 2))\n",
    "    x1, y1, x2, y2 = [], [], [], []\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            x1.append(points[:,0][i])\n",
    "            y1.append(points[:,1][i])\n",
    "            x2.append(points[:,0][j])\n",
    "            y2.append(points[:,1][j])\n",
    "\n",
    "    xs[:,0] = x1\n",
    "    xs[:,1] = y1\n",
    "    ys[:,0] = x2\n",
    "    ys[:,1] = y2\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1584ba2-95b3-4c52-9557-096cde72cefa",
   "metadata": {},
   "source": [
    "We get the boundary conditions as follows.\n",
    "To estimate $\\pi(x)$ for $x \\in \\mathbb{R}^2$ let for $i \\le j$\n",
    "$$ w_{ij} = \\exp\\left( - (|x-x_i|^2 + |x_j - x|^2) / 2 \\sigma^2_e - |x_i - x_j|^2 / 2 \\epsilon^2 \\right), $$\n",
    "where $\\sigma_e$ is four times the mean second-nearest-neighbor distance and $\\epsilon$ is a parameter.\n",
    "Then, using the diverences and diversities $d(i, j)$,\n",
    "get\n",
    "$$ \\hat \\pi(x) = \\frac{ \\sum_{i \\le j} d(i, j) w_{ij} }{ \\sum_{i \\le j} w_{ij} } . $$\n",
    "Then, for a point $(x, y) \\in \\mathbb{R}^2 \\times \\mathbb{R}^2$,\n",
    "let\n",
    "$$ \\hat d(x, y) = \\hat \\pi((x+y)/2) . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4130c07-6d82-4aba-8476-5f9778d77361",
   "metadata": {},
   "source": [
    "**Note:** the convergence of the solver below can depend on these parameters!\n",
    "TODO: figure out how to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82b1d01-b780-42e6-a747-768124e262d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.sqrt(np.subtract.outer(points[:,0], points[:,0])**2 + np.subtract.outer(points[:,1], points[:,1])**2)\n",
    "sigma_e = 4 * np.mean(dists[np.arange(20), dists.argsort()[:,2]]) # third-largest, row-wise\n",
    "xs, ys = coords_to_pairs(points)\n",
    "def kernl(x, y):\n",
    "    x = np.add(x, y) / 2\n",
    "    def _dists(x_i, x_j, x):\n",
    "        diffs = (x_i - x_j) / epsilon, (x_i - x) / sigma_e, (x_j - x) / sigma_e\n",
    "        dists = np.sum([np.hypot(*xx) for xx in diffs])\n",
    "        return dists\n",
    "    dists = np.array([_dists(x_i, y_i, x) for x_i, y_i in zip(xs, ys)])\n",
    "    scale = 2 / scaling['xy']**2\n",
    "    return np.exp(-dists / scale)\n",
    "\n",
    "def boundary(x, y):\n",
    "    k = kernl(x, y)\n",
    "    return (train_dxy * k).sum() / k.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e476b7d-7f1a-41ad-91ce-e8a549687070",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb39f3-7198-4d72-8ab5-c2ebc30f3278",
   "metadata": {},
   "source": [
    "Now we set up and solve the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254adcd-7e01-4ae3-970d-2c0a21bf6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqn = pf.HittingTimes(W, boundary, epsilon=epsilon)\n",
    "control = eqn.control\n",
    "\n",
    "# loss functionals\n",
    "reg = {'l2': [100*penalty, 100*penalty], 'smoothing': [penalty, penalty]}\n",
    "train_sd = pf.SpatialData(train_dxy, points, W)\n",
    "train_loss = pf.LossFunctional(train_sd, control, reg)\n",
    "\n",
    "invp = pf.InverseProblem(eqn, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a28f8c-d224-4fc8-99a5-f9aea29c4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'ftol': 1e-8, \n",
    "           'gtol': 1e-8, \n",
    "           'maxcor': 15,\n",
    "           'maxiter': 100}\n",
    "m_hats, losses, results = invp.optimize(control, \n",
    "                                        method='L-BFGS-B', \n",
    "                                        options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01148d92-e642-4dac-958e-ac874c6eea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.update(m_hats[-1])\n",
    "u_hat = eqn.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e9ba8-0c06-4439-8f50-5b8e3d8f79e5",
   "metadata": {},
   "source": [
    "Here is an example slice of the 4d solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacf903-134b-48ee-927d-6348cf300219",
   "metadata": {},
   "outputs": [],
   "source": [
    "fenics.plot(u_hat.get_slice((0.25, 0.5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e58087-98ee-4c0a-8d2d-54c57e790e52",
   "metadata": {},
   "source": [
    "And, finally, here are the inferred ellipse and vector fields\n",
    "for (reverse-time, effective) dispersal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063c0be-bd45-4a0f-947c-f12d0fa54830",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0, ax1 = eqn.plot_control()\n",
    "ax0.set_title(\"inferred sigma\")\n",
    "ax1.set_title(\"inferred bias\");\n",
    "Q = ax1.collections[0]\n",
    "ax1.quiverkey(Q, 0.9, 0.7, .002, label=f\"{.002}\", angle=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b34ea3-7c80-4fc5-ace4-1ae811856c34",
   "metadata": {},
   "source": [
    "# Comparison to the (reverse-time) truth\n",
    "\n",
    "Now we'll compute the \"truth\" as expected based on the recorded maps,\n",
    "which we first need to project into our function space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe5a04-1756-4ca3-b3c7-4db32fb7e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_fn = make_map_fn(empirical_maps, 'density')\n",
    "density = pf.transforms.callable_to_Function(lambda x, y: density_fn([[x * size, y * size]]), V)\n",
    "fenics.plot(density).axes.set_title(\"empirical density\");jjb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86484fc4-11f8-4324-a8cc-1d940691b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_fn = make_map_fn(empirical_maps, 'establishment')\n",
    "establishment = pf.transforms.callable_to_Function(lambda x, y: establishment_fn([[x * size, y * size]]), V)\n",
    "fenics.plot(establishment).axes.set_title(\"empirical establishment\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6ed46-7e71-415a-b214-4921f5017c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fecundity_fn = make_map_fn(empirical_maps, 'fecundity')\n",
    "fecundity = pf.transforms.callable_to_Function(lambda x, y: fecundity_fn([[x * size, y * size]]), V)\n",
    "fenics.plot(fecundity).axes.set_title(\"empirical fecundity\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e7ae2-0b4b-405c-b16a-a57a47de5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_total_fecundity = density.copy()\n",
    "log_total_fecundity.vector()[:] = np.log(0.1 + density.vector()[:] * fecundity.vector()[:])\n",
    "grad_log_total_fecundity = fenics.project(fenics.grad(log_total_fecundity))\n",
    "fenics.plot(grad_log_total_fecundity).axes.set_title(\"grad log total fecundity\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e39f1-db9f-4b2a-a521-301e07076ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_floats(x, min=-1, max=1):\n",
    "    \"\"\" The \"inverse\" of floats_to_rgb. Note the denominator is 255, mirroring SLiM. \"\"\"\n",
    "    out = min + (max - min) * np.array(x).astype('float') / 255\n",
    "    return out\n",
    "\n",
    "def image_fn(im, x, y, min=-1, max=1):\n",
    "    u = x * params['MAP_RESOLUTION'] * (1 - 1/im.width)\n",
    "    v = (im.height - y * params['MAP_RESOLUTION']) * (1 - 1/im.height)\n",
    "    z = im.getpixel((u, v))\n",
    "    return rgb_to_floats(z, min=min, max=max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db17d9a-0624-4788-8954-6a875dbb56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = pf.transforms.vectorized_fn(V, dim=2, name='bias')\n",
    "bias.vector()[:] = pf.transforms.callable_to_array(\n",
    "    lambda x, y: params['BIAS'] * image_fn(bias_map, x * size, y * size)[:2],\n",
    "    V,\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1123395-ff2a-487b-a380-5fac973e617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fenics.plot(bias).axes.set_title(\"forwards-time bias\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41138bb-171c-495a-a4ca-9b39846fcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bias = fenics.project(2 * establishment * fecundity * (grad_log_total_fecundity - bias))\n",
    "fenics.plot(true_bias).axes.set_title(\"true (reverse-time) bias\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
